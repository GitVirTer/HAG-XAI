classdef preluLayer < nnet.layer.Layer & nnet.layer.Formattable
    % Example custom PReLU layer.

    properties
        % (Optional) Layer properties.
    
        % Declare layer properties here.
        rawSize

    end

    properties (Learnable)
        % Layer learnable parameters.

        % Scaling coefficient.
        Alpha_act_pos
        Alpha_act_neg
        Alpha_grad_pos
        Alpha_grad_neg     

    end

    methods
        function layer = preluLayer(args)
            % layer = preluLayer creates a PReLU layer.
            %
            % layer = preluLayer(numChannels,Name=name) also specifies the
            % layer name.

            arguments
                args.Name = "";
            end

            % Set layer name.
            layer.Name = args.Name;

            % Set layer description.
            layer.Description = "PReLU";
            layer.rawSize = [576 1024];
        end

        function layer = initialize(layer,layout)
            % layer = initialize(layer,layout) initializes the learnable
            % parameters of the layer for the specified input layout.

            % Skip initialization of nonempty parameters.
            if (~isempty(layer.Alpha_act_pos)) && (~isempty(layer.Alpha_act_neg))
                return
            end

            % Input data size.
            sz = layout.Size;
            ndims = numel(sz);

            % Find number of channels.
            idx = finddim(layout,"C");
            numChannels = sz(idx);

            % Initialize Alpha.
            szAlpha = ones(1,ndims);
            szAlpha(idx) = 1; % =1: global ReLu. =numChannels: channel-wise ReLu
            layer.Alpha_act_pos = ones(szAlpha);
            layer.Alpha_act_neg = ones(szAlpha);
            layer.Alpha_grad_pos = ones(szAlpha);
            layer.Alpha_grad_neg = zeros(szAlpha);            
        end

        function Z = predict(layer, X)
            % Z = predict(layer, X) forwards the input data X through the
            % layer and outputs the result Z.
            eps_val = 1e-5;
            % X:SSCBT
            Act = X(:,:,:,:,1);
            Grads = X(:,:,:,:,2:end);

            Act_oper = layer.Alpha_act_pos .* max(0, Act) + layer.Alpha_act_neg .* min(0, Act);
            Grads_oper = layer.Alpha_grad_pos .* max(0, Grads) + layer.Alpha_grad_neg .* min(0, Grads);
            Act_Grad = Act_oper.*Grads_oper;    % SSCBT
            Act_Grad = sum(Act_Grad, finddim(Act_Grad,"C"));    
            Act_Grad = max(0, Act_Grad);    % ReLu
            % Normalization
            Act_Grad = (Act_Grad-min(Act_Grad,[],finddim(Act_Grad,"S")))./...
                (max(Act_Grad,[],finddim(Act_Grad,"S"))-min(Act_Grad,[],finddim(Act_Grad,"S"))+eps_val);
            saliencyMap = sum(Act_Grad, finddim(Act_Grad,"T"));
            saliencyMap = dlarray(saliencyMap, 'SSCB');
            saliencyMap = dlresize(saliencyMap,'OutputSize',layer.rawSize);
            % Flatten
            saliencyMap = stripdims(saliencyMap);
            saliencyMap = permute(saliencyMap, [4 1 2 3]);
            saliencyMap = saliencyMap(:,:); % BC
            saliencyMap = saliencyMap';     % CB
            

        end
    end
end




